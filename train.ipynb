{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading essential modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "import importlib\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning import LightningModule,Trainer\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "import models\n",
    "importlib.reload(models)\n",
    "from models import ResNet1D, Bio, Conv1D_v2, EEGInceptionModel, ChronoNet\n",
    "from dataset import EEG_inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instantiating the lightingmodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LModel(LightningModule):\n",
    "    def __init__(self, attribute):\n",
    "        super(LModel, self).__init__()\n",
    "        self.attribute = attribute\n",
    "        self.model = attribute[\"model\"] # initialize the model\n",
    "        self.lr = attribute[\"lr\"]\n",
    "        self.bs = 64\n",
    "        self.worker = 1\n",
    "        self.acc = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=0.0005)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = self.attribute[\"train_dataset\"]\n",
    "        return DataLoader(dataset, batch_size=self.bs, num_workers=self.worker, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = self.attribute[\"val_dataset\"]\n",
    "        return DataLoader(dataset, batch_size=self.bs, num_workers=self.worker, shuffle=False)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        signal, label = batch\n",
    "        out = self(signal.float())\n",
    "        loss = self.criterion(out.flatten(), label.float().flatten())\n",
    "        preds = (torch.sigmoid(out.flatten()) > 0.5).long()\n",
    "        acc = self.acc(preds, label.long().flatten())\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        signal, label = batch\n",
    "        out = self(signal.float())\n",
    "        loss = self.criterion(out.flatten(), label.float().flatten())\n",
    "        preds = (torch.sigmoid(out.flatten()) > 0.5).long()\n",
    "        acc = self.acc(preds, label.long().flatten())\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the all the combinations of dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 856/856 [00:00<00:00, 1873.36it/s]\n",
      "100%|██████████| 7650/7650 [00:04<00:00, 1859.44it/s]\n",
      "100%|██████████| 7650/7650 [00:04<00:00, 1843.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before (7650, 6000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7650/7650 [00:04<00:00, 1803.22it/s]\n",
      "100%|██████████| 19384/19384 [02:23<00:00, 134.83it/s]\n",
      "100%|██████████| 7650/7650 [00:03<00:00, 1918.79it/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset =  EEG_inception(kind = \"val\", normalize= 1)\n",
    "train_dataset_smote = EEG_inception(kind=\"train\", normalize= 1, balancing=\"smote\")\n",
    "\n",
    "# train_dataset is the signal augmentation dataset based on the paper EEG inception\n",
    "train_dataset = EEG_inception(kind=\"train\", normalize= 1, balancing=\"inception\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of ResNet1D model: torch.Size([3, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zokov\\.conda\\envs\\py3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Bio model: torch.Size([3, 1])\n",
      "Output of Conv1D_v2 model: torch.Size([3, 1])\n",
      "Output of SimplifiedEEGInceptionModel: torch.Size([3, 1])\n",
      "Output of ChronoNet model: torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# sanity for all the models\n",
    "\n",
    "# Generate a random input tensor of shape (batch_size=3, channels=8, time_steps=900)\n",
    "x = torch.randn(3, 8, 750)\n",
    "\n",
    "# Initialize an empty list to store the models\n",
    "models = []\n",
    "\n",
    "# 1. Sanity check for ResNet1D\n",
    "model_1 = ResNet1D()\n",
    "models.append(model_1)\n",
    "output_1 = model_1(x)\n",
    "print(f\"Output of ResNet1D model: {output_1.shape}\")\n",
    "\n",
    "# 2. Sanity check for Bio model\n",
    "model_3 = Bio(input_size=8)\n",
    "models.append(model_3)\n",
    "output_3 = model_3(x)\n",
    "print(f\"Output of Bio model: {output_3.shape}\")\n",
    "\n",
    "# 3. Sanity check for Conv1D_v2\n",
    "model_4 = Conv1D_v2(channels=8)\n",
    "models.append(model_4)\n",
    "output_4 = model_4(x)\n",
    "print(f\"Output of Conv1D_v2 model: {output_4.shape}\")\n",
    "\n",
    "# 4. Sanity check for SimplifiedEEGInceptionModel\n",
    "model_5 = EEGInceptionModel(in_channels=8)\n",
    "models.append(model_5)\n",
    "output_5 = model_5(x)\n",
    "print(f\"Output of SimplifiedEEGInceptionModel: {output_5.shape}\")\n",
    "\n",
    "# 5. Sanity check for ChronoNet\n",
    "model_6 = ChronoNet(8)\n",
    "models.append(model_6)\n",
    "output_6 = model_6(x)\n",
    "print(f\"Output of ChronoNet model: {output_6.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organizing all combinations of models for a single run. \n",
    "\n",
    "attributes = {\n",
    "    1: {\"model\": ResNet1D(), \"train_dataset\": train_dataset, \"val_dataset\": val_dataset, \"lr\": 0.0001},\n",
    "    2: {\"model\": ResNet1D(), \"train_dataset\": train_dataset_smote, \"val_dataset\": val_dataset, \"lr\": 0.0001},\n",
    "    3: {\"model\": ResNet1D(), \"train_dataset\": train_dataset, \"val_dataset\": val_dataset, \"lr\": 0.0005},\n",
    "    4: {\"model\": ResNet1D(), \"train_dataset\": train_dataset_smote, \"val_dataset\": val_dataset, \"lr\": 0.0005},\n",
    "    \n",
    "    5: {\"model\": Bio(input_size=8), \"train_dataset\": train_dataset, \"val_dataset\": val_dataset, \"lr\": 0.0001},\n",
    "    6: {\"model\": Bio(input_size=8), \"train_dataset\": train_dataset_smote, \"val_dataset\": val_dataset, \"lr\": 0.0001},\n",
    "    7: {\"model\": Bio(input_size=8), \"train_dataset\": train_dataset, \"val_dataset\": val_dataset, \"lr\": 0.0005},\n",
    "    8: {\"model\": Bio(input_size=8), \"train_dataset\": train_dataset_smote, \"val_dataset\": val_dataset, \"lr\": 0.0005},\n",
    "    \n",
    "    9: {\"model\": Conv1D_v2(channels=8), \"train_dataset\": train_dataset, \"val_dataset\": val_dataset, \"lr\": 0.0001},\n",
    "    10: {\"model\": Conv1D_v2(channels=8), \"train_dataset\": train_dataset_smote, \"val_dataset\": val_dataset, \"lr\": 0.0001},\n",
    "    11: {\"model\": Conv1D_v2(channels=8), \"train_dataset\": train_dataset, \"val_dataset\": val_dataset, \"lr\": 0.0005},\n",
    "    12: {\"model\": Conv1D_v2(channels=8), \"train_dataset\": train_dataset_smote, \"val_dataset\": val_dataset, \"lr\": 0.0005},\n",
    "    \n",
    "    13: {\"model\": EEGInceptionModel(in_channels=8), \"train_dataset\": train_dataset, \"val_dataset\": val_dataset, \"lr\": 0.0001},\n",
    "    14: {\"model\": EEGInceptionModel(in_channels=8), \"train_dataset\": train_dataset_smote, \"val_dataset\": val_dataset, \"lr\": 0.0001},\n",
    "    15: {\"model\": EEGInceptionModel(in_channels=8), \"train_dataset\": train_dataset, \"val_dataset\": val_dataset, \"lr\": 0.0005},\n",
    "    16: {\"model\": EEGInceptionModel(in_channels=8), \"train_dataset\": train_dataset_smote, \"val_dataset\": val_dataset, \"lr\": 0.0005},\n",
    "    \n",
    "    17: {\"model\": ChronoNet(channel=8), \"train_dataset\": train_dataset, \"val_dataset\": val_dataset, \"lr\": 0.0001},\n",
    "    18: {\"model\": ChronoNet(channel=8), \"train_dataset\": train_dataset_smote, \"val_dataset\": val_dataset, \"lr\": 0.0001},\n",
    "    19: {\"model\": ChronoNet(channel=8), \"train_dataset\": train_dataset, \"val_dataset\": val_dataset, \"lr\": 0.0005},\n",
    "    20: {\"model\": ChronoNet(channel=8), \"train_dataset\": train_dataset_smote, \"val_dataset\": val_dataset, \"lr\": 0.0005}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_key, attribute in attributes.items():\n",
    "    print(f\"{attribute['train_dataset']}_{attribute['lr']}\")\n",
    "\n",
    "    model_name = attribute[\"model\"].__class__.__name__\n",
    "    print(model_name, \"here\")\n",
    "\n",
    "    # Extract the learning rate (if present)\n",
    "    lr = attribute.get(\"lr\", None) \n",
    "    \n",
    "    # Extract the train and validation datasets\n",
    "    train_dataset = attribute[\"train_dataset\"]\n",
    "    val_dataset = attribute[\"val_dataset\"]\n",
    "    \n",
    "    # Determine the dataset type (train_dataset_smote or train_dataset)\n",
    "    dataset_type = 'train_dataset_smote' if train_dataset == train_dataset_smote else 'train_dataset'\n",
    "\n",
    "    # Define EarlyStopping callback\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience = 6,\n",
    "        verbose=True,\n",
    "        mode='max',\n",
    "        check_finite=True\n",
    "    )\n",
    "\n",
    "    print(model_name, \"here\")\n",
    "    # Modify ModelCheckpoint callback to use a custom filename with placeholders\n",
    "    \n",
    "    \n",
    "    # Create a model name based on the above information\n",
    "    model_identifier = f\"{model_name}_lr_{lr}_dataset_{dataset_type}\"\n",
    "    \n",
    "    model = attribute[\"model\"]\n",
    "    lr = attribute[\"lr\"]\n",
    "    train_dataset = attribute[\"train_dataset\"]\n",
    "    dataset_type = 'train_dataset_smote' if train_dataset == train_dataset_smote else 'train_dataset_inception'\n",
    "\n",
    "    # Print the model identifier for clarity\n",
    "    print(f\"Training model: {model_identifier}\")\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=f\"checkpoints_v3(1)_adamw_l2_0.0004_demo/{model_identifier}\",\n",
    "        filename='{epoch}_v{val_acc:.4f}_t{train_acc:.4f}',\n",
    "        save_top_k=3,\n",
    "        verbose=True,\n",
    "        monitor='val_acc',\n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=200,\n",
    "        callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model = LModel(attribute)\n",
    "    trainer.fit(model)\n",
    "    print(trainer.callback_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
