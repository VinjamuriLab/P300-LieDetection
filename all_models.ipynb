{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchmetrics\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning import LightningModule,Trainer\n",
    "from torch.utils.data import TensorDataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EEG_inception_dataset import EEG_inception\n",
    "from model import Conv1D_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chrononet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, inplace):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=inplace, out_channels=16, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv1d(in_channels=inplace, out_channels=16, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=inplace, out_channels=16, kernel_size=8, stride=2, padding=3)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.relu(self.conv1(x))\n",
    "        x2 = self.relu(self.conv2(x))\n",
    "        x3 = self.relu(self.conv3(x))\n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "        return x\n",
    "\n",
    "class ChronoNet(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super().__init__()\n",
    "        # Use fewer blocks and channels\n",
    "        self.block1 = Block(channel)\n",
    "        self.block2 = Block(48)  # Adjust input to match output channels from Block\n",
    "        \n",
    "        # Use a single GRU layer to simplify the model\n",
    "        self.gru = nn.GRU(input_size=48, hidden_size=32, batch_first=True)\n",
    "        \n",
    "        # Linear layer for output prediction\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = x.permute(0, 2, 1)  # Adjust for GRU input\n",
    "        \n",
    "        # GRU forward pass\n",
    "        gru_out, _ = self.gru(x)\n",
    "        # Use the last output from GRU\n",
    "        x = gru_out[:, -1, :]  \n",
    "        \n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4306],\n",
      "        [0.4347],\n",
      "        [0.4354]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input=torch.randn(3,8,1000)\n",
    "input.shape\n",
    "model=ChronoNet(8)\n",
    "out=model(input)\n",
    "out.shape\n",
    "print(torch.sigmoid(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instantiating the lighting module for the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "\n",
    "class ChronoModel(LightningModule):\n",
    "    def __init__(self, attribute):\n",
    "        super(ChronoModel, self).__init__()\n",
    "        self.attribute = attribute\n",
    "        self.model = attribute[\"model\"] # initialize the model\n",
    "        self.lr = 1e-4\n",
    "        self.bs = 64\n",
    "        self.worker = 2\n",
    "        self.acc = torchmetrics.Accuracy(task=\"binary\")\n",
    "     \n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = self.attribute[\"train_dataset\"]\n",
    "        return DataLoader(dataset, batch_size=self.bs, num_workers=self.worker, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = self.attribute[\"val_dataset\"]\n",
    "        return DataLoader(dataset, batch_size=self.bs, num_workers=self.worker, shuffle=False)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        signal, label = batch\n",
    "        out = self(signal.float())\n",
    "        loss = self.criterion(out.flatten(), label.float().flatten())\n",
    "        preds = (torch.sigmoid(out.flatten()) > 0.5).long()\n",
    "        # print(preds.)\n",
    "        acc = self.acc(preds, label.long().flatten())\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        signal, label = batch\n",
    "        out = self(signal.float())\n",
    "        loss = self.criterion(out.flatten(), label.float().flatten())\n",
    "        preds = (torch.sigmoid(out.flatten()) > 0.5).long()\n",
    "        acc = self.acc(preds, label.long().flatten())\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # Retrieve the average training loss\n",
    "        train_loss = self.trainer.callback_metrics.get(\"train_loss_epoch\", None)\n",
    "        if train_loss is not None:\n",
    "            self.print(f\"Epoch {self.current_epoch} - Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Retrieve the average validation loss\n",
    "        val_loss = self.trainer.callback_metrics.get(\"val_loss\", None)\n",
    "        if val_loss is not None:\n",
    "            self.print(f\"Epoch {self.current_epoch} - Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should be here \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4988/4988 [00:32<00:00, 153.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4988, 8, 750) shap[e]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7650/7650 [00:11<00:00, 661.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4988, 8, 750) in here dataset\n",
      "train main_job done 10790 4988 4988\n",
      "should be here \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 856/856 [00:07<00:00, 107.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(856, 8, 750) shap[e]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7650/7650 [00:04<00:00, 1850.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(856, 8, 750) in here dataset\n",
      "val main_job done 1350 856 856\n"
     ]
    }
   ],
   "source": [
    "attributes = {1 : {\"model\" : ChronoNet(channel=8), \"train_dataset\" : EEG_inception(kind=\"train\", normalize= 1, balancing=\"equal_samples\"), \"val_dataset\" : EEG_inception(kind = \"val\", normalize= 1)}}\n",
    "\n",
    "model=ChronoModel(attribute = attributes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer=Trainer(max_epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type              | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | model     | ChronoNet         | 21.1 K | train\n",
      "1 | acc       | BinaryAccuracy    | 0      | train\n",
      "2 | criterion | BCEWithLogitsLoss | 0      | train\n",
      "--------------------------------------------------------\n",
      "21.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "21.1 K    Total params\n",
      "0.084     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d379e95000f4abaa9017c8106a29022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zokov\\.conda\\envs\\py3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Validation Loss: 0.6934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zokov\\.conda\\envs\\py3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38fe855c9e2454da6b12fc095dd219d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeab3f35435a4d9dbaeed35af291f65a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Validation Loss: 0.6932\n",
      "Epoch 0 - Training Loss: 0.6925\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec9490a55c34a3d8fb9cd651020780c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Validation Loss: 0.6933\n",
      "Epoch 1 - Training Loss: 0.6915\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bf2d56aefd4b31a9a4690f63fbf52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Validation Loss: 0.6935\n",
      "Epoch 2 - Training Loss: 0.6906\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2bd1215f464f3fb24cefa8ec2441a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Validation Loss: 0.6940\n",
      "Epoch 3 - Training Loss: 0.6900\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5bb77471064ac391a9a94498fc7f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Validation Loss: 0.6941\n",
      "Epoch 4 - Training Loss: 0.6892\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b3de189bfe4405a01a197361ca7e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Validation Loss: 0.6946\n",
      "Epoch 5 - Training Loss: 0.6886\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0745c138034a424ca83cbde0f52b74d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Validation Loss: 0.6951\n",
      "Epoch 6 - Training Loss: 0.6881\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8761356334e54bcb931adb44d098ebc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Validation Loss: 0.6955\n",
      "Epoch 7 - Training Loss: 0.6874\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffadff56fd04b558b932c765e56110b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Validation Loss: 0.6958\n",
      "Epoch 8 - Training Loss: 0.6868\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c24ca90b71741988b90f840eff06710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Validation Loss: 0.6967\n",
      "Epoch 9 - Training Loss: 0.6865\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053c775c8d144405993b30cf4cb4a79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Validation Loss: 0.6972\n",
      "Epoch 10 - Training Loss: 0.6857\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7276a6b31b14d9c8186942b99a41b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Validation Loss: 0.6979\n",
      "Epoch 11 - Training Loss: 0.6850\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb15ea1529141f9b1849647140530fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Validation Loss: 0.6988\n",
      "Epoch 12 - Training Loss: 0.6844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b513ce72fc1247108aa7809f6fe60c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Validation Loss: 0.6996\n",
      "Epoch 13 - Training Loss: 0.6838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b5c5e2939c439e81361fd13e9df337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Validation Loss: 0.7010\n",
      "Epoch 14 - Training Loss: 0.6832\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03afdf321f6345c8912bd9aabe051f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Validation Loss: 0.7015\n",
      "Epoch 15 - Training Loss: 0.6824\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab17084413441a582dc6a32900c9b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Validation Loss: 0.7024\n",
      "Epoch 16 - Training Loss: 0.6820\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a47f06de0243f89e9e146caaef175e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Validation Loss: 0.7037\n",
      "Epoch 17 - Training Loss: 0.6815\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4613b61b6391411e96c8b2f4e9a9522e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Validation Loss: 0.7043\n",
      "Epoch 18 - Training Loss: 0.6806\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf519ea26524ffab2e4976001c30a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Validation Loss: 0.7076\n",
      "Epoch 19 - Training Loss: 0.6802\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c73b49638d45e2a355576f69a1d9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Validation Loss: 0.7082\n",
      "Epoch 20 - Training Loss: 0.6793\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c64c949642b4b55b0a19efd0ddff282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Validation Loss: 0.7075\n",
      "Epoch 21 - Training Loss: 0.6788\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a480ea4c4d4c4a943e57a4a4d40926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Validation Loss: 0.7071\n",
      "Epoch 22 - Training Loss: 0.6793\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d08a62db13a407ab4a388e316475886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Validation Loss: 0.7093\n",
      "Epoch 23 - Training Loss: 0.6775\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2472162cf2d46c6ad1af8ea19490a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Validation Loss: 0.7109\n",
      "Epoch 24 - Training Loss: 0.6768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feaefb92be624039a2e4a2f5925ae4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Validation Loss: 0.7115\n",
      "Epoch 25 - Training Loss: 0.6758\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00229083f624fcbb0ae14f89d6fc01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Validation Loss: 0.7139\n",
      "Epoch 26 - Training Loss: 0.6753\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4160e46ad8dd427b979959af7146014f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Validation Loss: 0.7155\n",
      "Epoch 27 - Training Loss: 0.6738\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e96e3cf71e0423e96aa32658b8d4f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Validation Loss: 0.7165\n",
      "Epoch 28 - Training Loss: 0.6731\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': tensor(0.6919), 'train_loss_step': tensor(0.7212), 'train_acc': tensor(0.5230), 'train_acc_step': tensor(0.3750), 'val_loss': tensor(0.6909), 'val_acc': tensor(0.5275), 'train_loss_epoch': tensor(0.6919), 'train_acc_epoch': tensor(0.5230)}\n"
     ]
    }
   ],
   "source": [
    "print(trainer.callback_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8, 1000) in here dataset\n",
      "train kind 1000 1000 1000\n",
      "main_job done\n"
     ]
    }
   ],
   "source": [
    "train_data = EEG_inception(kind=\"train\", normalize = False)\n",
    "dl = DataLoader(train_data, batch_size=64, num_workers=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5210)\n"
     ]
    }
   ],
   "source": [
    "pred_label = []\n",
    "true_label = []\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "for ii, (data, label) in enumerate(dl):\n",
    "    input = data.float().to(device)\n",
    "    label = label.to(device)\n",
    "\n",
    "    pred = model(input).float()\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = (pred >= 0.5).float().to(device).data\n",
    "\n",
    "    pred = pred.view(-1)\n",
    "    \n",
    "    pred_label.append(pred)\n",
    "    true_label.append(label)\n",
    "    # print(pred, \"val\")\n",
    "    # print(label)\n",
    "pred_label = torch.cat(pred_label, 0)\n",
    "true_label = torch.cat(true_label, 0)\n",
    "\n",
    "val_accuracy = torch.sum(pred_label == true_label).type(torch.FloatTensor) / true_label.size(0)\n",
    "\n",
    "print(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ones = torch.sum(pred_label == 0).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2559"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(pred_label == 1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (818234999.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[15], line 12\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.conv3 = nn.Conv1d(in_channels=inplace, out_channels=32, kernel_size=8, stride=2, ====padding=3)\u001b[0m\n\u001b[1;37m                                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, inplace):\n",
    "        super().__init__()\n",
    "        # Add batch normalization and proper initialization\n",
    "        self.conv1 = nn.Conv1d(in_channels=inplace, out_channels=32, kernel_size=2, stride=2, padding=0)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.conv2 = nn.Conv1d(in_channels=inplace, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.conv3 = nn.Conv1d(in_channels=inplace, out_channels=32, kernel_size=8, stride=2, padding=3)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.dropout(self.relu(self.bn1(self.conv1(x))))\n",
    "        x2 = self.dropout(self.relu(self.bn2(self.conv2(x))))\n",
    "        x3 = self.dropout(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "        return x\n",
    "\n",
    "class ChronoNet(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super().__init__()\n",
    "        self.block1 = Block(channel)\n",
    "        self.block2 = Block(96)\n",
    "        self.block3 = Block(96)\n",
    "        \n",
    "        # Add dropout and batch normalization to GRU layers\n",
    "        self.gru1 = nn.GRU(input_size=96, hidden_size=32, batch_first=True, dropout=0.2)\n",
    "        self.gru2 = nn.GRU(input_size=32, hidden_size=32, batch_first=True, dropout=0.2)\n",
    "        self.gru3 = nn.GRU(input_size=64, hidden_size=32, batch_first=True, dropout=0.2)\n",
    "        self.gru4 = nn.GRU(input_size=96, hidden_size=32, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        self.gru_linear = nn.Linear(225, 1)\n",
    "        self.bn_linear = nn.BatchNorm1d(32)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # Initialize the linear layers\n",
    "        nn.init.xavier_normal_(self.gru_linear.weight)\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        gru_out1, _ = self.gru1(x)\n",
    "        gru_out2, _ = self.gru2(gru_out1)\n",
    "        gru_out = torch.cat([gru_out1, gru_out2], dim=2)\n",
    "        \n",
    "        gru_out3, _ = self.gru3(gru_out)\n",
    "        gru_out = torch.cat([gru_out1, gru_out2, gru_out3], dim=2)\n",
    "        \n",
    "        linear_out = self.dropout(self.relu(self.gru_linear(gru_out.permute(0, 2, 1))))\n",
    "        gru_out4, _ = self.gru4(linear_out.permute(0, 2, 1))\n",
    "        \n",
    "        x = self.flatten(gru_out4)\n",
    "        x = self.bn_linear(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class ChronoModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(ChronoModel, self).__init__()\n",
    "        self.model = ChronoNet(8)\n",
    "        self.lr = 1e-3  # Increased learning rate\n",
    "        self.bs = 32    # Reduced batch size\n",
    "        self.worker = 2\n",
    "        self.acc = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.0]))  # Balanced weight\n",
    "        \n",
    "        # Add early stopping metrics\n",
    "        self.best_val_acc = 0.0\n",
    "        self.patience_counter = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.1, patience=3, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_acc\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        signal, label = batch\n",
    "        out = self(signal.float())\n",
    "        loss = self.criterion(out.flatten(), label.float().flatten())\n",
    "        \n",
    "        # Add gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
    "        \n",
    "        preds = torch.sigmoid(out.flatten())\n",
    "        acc = self.acc(preds, label.long().flatten())\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        signal, label = batch\n",
    "        out = self(signal.float())\n",
    "        loss = self.criterion(out.flatten(), label.float().flatten())\n",
    "        \n",
    "        preds = torch.sigmoid(out.flatten())\n",
    "        acc = self.acc(preds, label.long().flatten())\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\zokov\\.conda\\envs\\py3\\Lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\zokov\\.conda\\envs\\py3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "\n",
      "  | Name      | Type              | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | model     | ChronoNet         | 131 K  | train\n",
      "1 | acc       | BinaryAccuracy    | 0      | train\n",
      "2 | criterion | BCEWithLogitsLoss | 0      | train\n",
      "--------------------------------------------------------\n",
      "131 K     Trainable params\n",
      "0         Non-trainable params\n",
      "131 K     Total params\n",
      "0.526     Total estimated model params size (MB)\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "`train_dataloader` must be implemented to be used with the Lightning Trainer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 11\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, ModelCheckpoint\n\u001b[0;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      5\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m      6\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     ]\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 11\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 543\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    545\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    575\u001b[0m     ckpt_path,\n\u001b[0;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    578\u001b[0m )\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    991\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1030\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1030\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m   1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:197\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_data()\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:225\u001b[0m, in \u001b[0;36m_FitLoop.setup_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    222\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: resetting train dataloader\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    224\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_source\n\u001b[1;32m--> 225\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m _request_dataloader(source)\n\u001b[0;32m    226\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mbarrier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_dataloader()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_dataloader, CombinedLoader):\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:325\u001b[0m, in \u001b[0;36m_request_dataloader\u001b[1;34m(data_source)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Requests a dataloader by calling dataloader hooks corresponding to the given stage.\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03m    The requested dataloader\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \n\u001b[0;32m    319\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _replace_dunder_methods(DataLoader, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m), _replace_dunder_methods(BatchSampler):\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;66;03m# under this context manager, the arguments passed to `DataLoader.__init__` will be captured and saved as\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;66;03m# attributes on the instance in case the dataloader needs to be re-instantiated later by Lightning.\u001b[39;00m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;66;03m# Also, it records all attribute setting and deletion using patched `__setattr__` and `__delattr__`\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;66;03m# methods so that the re-instantiated object is as close to the original as possible.\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_source\u001b[38;5;241m.\u001b[39mdataloader()\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:289\u001b[0m, in \u001b[0;36m_DataLoaderSource.dataloader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the dataloader from the source.\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \n\u001b[0;32m    285\u001b[0m \u001b[38;5;124;03mIf the source is a module, the method with the corresponding :attr:`name` gets called.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \n\u001b[0;32m    287\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance\u001b[38;5;241m.\u001b[39mtrainer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, pl_module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance, pl\u001b[38;5;241m.\u001b[39mLightningDataModule):\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:159\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 159\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    162\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\Lib\\site-packages\\pytorch_lightning\\core\\hooks.py:482\u001b[0m, in \u001b[0;36mDataHooks.train_dataloader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TRAIN_DATALOADERS:\n\u001b[0;32m    456\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"An iterable or collection of iterables specifying training samples.\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m    For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    480\u001b[0m \n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`train_dataloader` must be implemented to be used with the Lightning Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: `train_dataloader` must be implemented to be used with the Lightning Trainer"
     ]
    }
   ],
   "source": [
    "# Initialize and train\n",
    "model = ChronoModel()\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "trainer = Trainer(\n",
    "    max_epochs=50,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', patience=5),\n",
    "        ModelCheckpoint(monitor='val_acc', mode='max')\n",
    "    ]\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
